{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-JiQyfWJYklI",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "y-Ehk30pYrdP",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "YPEH6qLeZNRQ",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "TIqpNgepFxVj",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/treasure823/Credit_Card_Default_Prediction_Nidhi_Pandey/blob/main/Credit_Card_Default_Prediction_Nidhi_Pandey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Credit Card Default Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**   NIDHI PANDEY\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the K-S chart to evaluate which customers will default on their credit card paymentsThe financial institution considers issuing the client a credit card, the institution needs to\n",
        "check the payment history of that person because the decision on whether to pay on due\n",
        "or owe the bill on a specific month usually relates to the previous payment history. For\n",
        "instance, if a person owes numerous bills already, he or she is likely to delay the payment\n",
        "of the current month unless this person gets a windfall so that the total arrears can be\n",
        "paid off. Besides the payment history, it is also imperative to look at the applicants’ credit\n",
        "limit of their current credit cards. This is a result of a virtuous circle: people who pay on\n",
        "duly tend to have better credit scores, so the banks prefer to increase these people’s\n",
        "credit lines by taking less risk. As a result, if a potential client already has a credit card\n",
        "with a high credit limit line, this person is unlikely to fail to pay the full amount owed in\n",
        "the future. Although the financial institution often collects clients’ personal information\n",
        "such as age, educational level, and marital status when people apply for credit cards, this\n",
        "information also affects the default behavior. In other words, the financial institution\n",
        "should equally consider their potential clients who are men or women, obtain bachelor\n",
        "degrees or master degrees, single or married when deciding whether to approve their\n",
        "credit card/loan applications. We tried our best to make a thorough analysis, and there\n",
        "are still a few possible improvements that may require longer-term action. We tried to\n",
        "implement several models such as logistic regression, Support vector Classifier, Random\n",
        "Forest, XGBoost, but various variants of boosting techniques may also be utilized in the\n",
        "future. The financial market changes rapidly every day, and people’s economic status and\n",
        "performance are affected by the market all the time."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/treasure823/Credit_Card_Default_Prediction_Nidhi_Pandey"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the K-S chart to evaluate which customers will default on their credit card payments**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "\n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "\n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "\n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "\n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FnpBetOQ-Asr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Laoding the dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/Credit_Card_Default_Prediction_Nidhi_Pandey/default of credit card clients.xls',header = 1)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "z_W4IZ3IBMY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns.value_counts()"
      ],
      "metadata": {
        "id": "nBlRXsFzE4Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = len(df[df.duplicated()])\n",
        "print(duplicates)"
      ],
      "metadata": {
        "id": "k63b-IBSD78F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.notnull().count()"
      ],
      "metadata": {
        "id": "7LXFo0v7ZN1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"Attributes\", size=14, weight=\"bold\")\n",
        "plt.title(\"Percentage of Missing values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno\n",
        "msno.bar(df, color='green',sort='ascending', figsize=(10,5), fontsize=15)"
      ],
      "metadata": {
        "id": "lcnncz-ifF55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No missing or null values or duplicates are found in our dataset.**"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().transpose()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attribute Information:\n",
        "\n",
        "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:\n",
        "\n",
        "* X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
        "\n",
        "* X2: Gender (1 = male; 2 = female).\n",
        "\n",
        "* X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
        "\n",
        "* X4: Marital status (1 = married; 2 = single; 3 = others).\n",
        "\n",
        "* X5: Age (year).\n",
        "\n",
        "* X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
        "\n",
        "* X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.\n",
        "\n",
        "* X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.ID.unique()"
      ],
      "metadata": {
        "id": "EE5PimNlUCC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique())"
      ],
      "metadata": {
        "id": "imqONmArU0df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WltIa4QI9gRg"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "df.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "df.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "idtdHYMdbx-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "_P47y2t3b73D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(10)"
      ],
      "metadata": {
        "id": "5nvvMBUZcBPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().transpose()"
      ],
      "metadata": {
        "id": "bE1jUPwYcKhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have rename all the columns and then describe all the columns again with their new name .The insights we got from the above data is shown  in the above describe table."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.lineplot(data = df , x= 'AGE',  y = 'MARRIAGE')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python line charts are visual displays of data in which the datapoints are linked by lines. Line charts are frequently used to show sequential values that aid in trend identification.**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see that most of the marriages are performed   between the age of 20 t0  30**"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NO**"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "sns.scatterplot( data= df , x ='BILL_AMT_SEPT', y= 'PAY_AMT_SEPT', color = 'pink')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The main purposes of scatter plots are to examine and display correlations between two numerical variables. When the data are viewed as a whole, the patterns shown by the dots in a scatter plot are in addition to the values of the individual data points. With scatter plots, correlational link identification is prevalent.**"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The maximum amt in the payment bill is between 20000 to 40000.**"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sns.countplot( data = df , x='PAY_APR')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['PAY_APR'].value_counts()"
      ],
      "metadata": {
        "id": "5hgvVItMYcZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The occurrence(counts) of the observation present in the categorical variable are represented using a countplot. For the visual representation, it makes use of the idea of a bar chart.**"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The max count in 'pay_apr' is above 16000.**"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "sns.barplot( data = df , x='PAY_APR',y='BILL_AMT_APR')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **I have used bar graph for the better specification of the data .**"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bill amount is between 40000 to 60000**"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NO**"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.countplot(x = 'default payment next month', data = df)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['default payment next month'].value_counts()"
      ],
      "metadata": {
        "id": "lHXiNOFOWyX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The occurrence(counts) of the observation present in the categorical variable are represented using a countplot. For the visual representation, it makes use of the idea of a bar chart**"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKS0ZMrE8lFg"
      },
      "source": [
        "Here,\n",
        "\n",
        "`0 - Not Default`\n",
        "\n",
        "`1 - Default`\n",
        "\n",
        "so we can conclude that, Defaulters are less as compare to the Non-Defaulters in the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No there is no  negative growth can be seen from the given data .**"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "cITLBBjjYzp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"EDUCATION\"] = df[\"EDUCATION\"].replace({4:0,5:0,6:0})\n",
        "df[\"EDUCATION\"].value_counts()"
      ],
      "metadata": {
        "id": "aoQavXtkY_DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data= df , x='EDUCATION')"
      ],
      "metadata": {
        "id": "GahwzF3VZOb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The occurrence(counts) of the observation present in the categorical variable are represented using a countplot. For the visual representation, it makes use of the idea of a bar chart**"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The max no. education count is above 14000**"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "df['AGE'].value_counts()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13,12))\n",
        "sns.countplot(data= df , x='AGE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1HHgeF8KaLZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The occurrence(counts) of the observation present in the categorical variable are represented using a countplot. For the visual representation, it makes use of the idea of a bar chart**"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The max age of pepople is lying between 24 to 30 **"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "df['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"MARRIAGE\"] = df[\"MARRIAGE\"].replace({0:3})\n",
        "df[\"MARRIAGE\"].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "o6nUpH1zmCRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data =df , x = 'MARRIAGE')"
      ],
      "metadata": {
        "id": "gCDjml_vodTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The occurrence(counts) of the observation present in the categorical variable are represented using a countplot. For the visual representation, it makes use of the idea of a bar chart**"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More than 16000 people are married .**"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "df['LIMIT_BAL'].shape"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['LIMIT_BAL'], kde = True)"
      ],
      "metadata": {
        "id": "HRSHmvdLozn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One common graphing tool is the histogram. It is employed to present interval-scaled summaries of discrete or continuous data. It is frequently used to depict the key characteristics of the data distribution in an easy-to-read format.**"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"LIMIT_BAL\" lies between 2500 to 3500**"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "sns.barplot(x='default payment next month', y='LIMIT_BAL', data=df)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The occurrence(counts) of the observation present in the categorical variable are represented using a countplot. For the visual representation, it makes use of the idea of a bar chart**"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Default payment in next month is above 175000**"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "ax = sns.boxplot(x=\"default payment next month\", y=\"LIMIT_BAL\", data = df)"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Box Plot is also known as Whisker plot is created to display the summary of the set of data values having properties like minimum, first quartile, median, third quartile and maximum.**"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The visualization of meadian is done by the use of the boxplot.**"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "sns.distplot(df[df['MARRIAGE']==1] ['AGE'],color = 'green', label= 'MARRIAGE')\n",
        "sns.distplot(df[df['MARRIAGE']==0] ['AGE'],color = 'pink', label= 'NO MARRIAGE');\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used distplot for comparing two values."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THe mgraph shows the comparision between married and not married.**"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "pay_col = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR']\n",
        "for col in pay_col:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.countplot(x = col, hue = 'default payment next month', data = df)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The occurrence(counts) of the observation present in the categorical variable are represented using a countplot. For the visual representation, it makes use of the idea of a bar chart**"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The insights can be seen with the help of the 0 and 1 for the default payment next month.**\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No**"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(15,12))\n",
        "sns.heatmap(df.corr(),cmap='PiYG',annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find and remove correlated features\n",
        "\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()                                           # Set of all the names of correlated features\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:        # we are interested in absolute coeff value\n",
        "                colname = corr_matrix.columns[i]               # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n"
      ],
      "metadata": {
        "id": "EwBzvag6V1m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the highly correlated features\n",
        "correlation(df, 0.9)          # setting threshold of 0.9"
      ],
      "metadata": {
        "id": "LYl46VTQV9MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The strength and direction of the linear links between two sets of variables are evaluated using correlation coefficients. Calculating the pairwise correlation coefficients between two or more (numeric) variables is made simple by using a correlation matrix.\n",
        "\n",
        "The correlation coefficient is a numerical measure of the strength and direction of a linear relationship between two variables. In other words, it measures the extent to which changes in one variable are associated with changes in the other variable. The correlation coefficient ranges from -1 to 1, with -1 indicating a perfect negative correlation, 1 indicating a perfect positive correlation, and 0 indicating no correlation.\n",
        "\n",
        "The correlation coefficient is an important tool in data analysis and machine learning, as it can help to identify relationships between variables and can be used in feature selection techniques to remove highly correlated features, which can reduce overfitting and improve the performance of the model."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'BILL_AMT_APR',\n",
        " 'BILL_AMT_AUG',\n",
        " 'BILL_AMT_JUL',\n",
        " 'BILL_AMT_JUN',\n",
        " 'BILL_AMT_MAY'} the correlation shown ."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "bill_amnt_df = df[['BILL_AMT_SEPT', 'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY', 'BILL_AMT_APR']]\n",
        "sns.pairplot(data = bill_amnt_df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pay_amnt_df = df[['PAY_AMT_SEPT',\t'PAY_AMT_AUG',\t'PAY_AMT_JUL',\t'PAY_AMT_JUN',\t'PAY_AMT_MAY',\t'PAY_AMT_APR', 'default payment next month']]\n",
        "\n",
        "sns.pairplot(data = pay_amnt_df, hue='default payment next month')"
      ],
      "metadata": {
        "id": "R1A6fEC6c5yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Pair plots are used to determine the most distinct clusters or the best combination of features to describe a connection between two variables. By creating some straightforward linear separations or basic lines in our data set, it also helps to create some straightforward classification models.**"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bill amt of each month is related with each other . And pay_amt of each month is related  to each other it can be seen through the pair plot.**"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis in Statistics**\n",
        "\n",
        "Hypothesis is an assumption about a parameter in population.\n",
        "\n",
        "**Null Hypothesis**\n",
        "\n",
        "It assumes that the observation is not statistically significant.\n",
        "\n",
        "**Alternate Hypothesis**\n",
        "\n",
        "It assumes that the observations are due to some reason.\n",
        "\n",
        "It's alternate to Null Hypothesis.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "For an assessment of a student we would take:\n",
        "\n",
        "\"student is worse than average\" - as a null hypothesis, and:\n",
        "\n",
        "\"student is better than average\" - as an alternate hypothesis"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "sns.displot(data=df, x=\"LIMIT_BAL\", hue=\"default payment next month\", multiple=\"stack\", stat='count', kde=True, line_kws={'ls':'--', 'lw':2}, height=5, aspect=2). \\\n",
        "    set_axis_labels(\"LIMIT_BAL\", \"default payment next month\")\n",
        "plt.ticklabel_format(style=\"plain\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "v1 = np.random.normal(size=100)\n",
        "v2 = np.random.normal(size=100)\n",
        "\n",
        "res = ttest_ind(v1, v2)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "AoIIcBwEY_dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T-Test**\n",
        "\n",
        "T-tests are used to determine if there is significant deference between means of two variables and lets us know if they belong to the same distribution.\n",
        "\n",
        "It is a two tailed test.\n",
        "\n",
        "The function ttest_ind() takes two samples of same size and produces a tuple of t-statistic and p-value"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-tests are used to determine if there is significant deference between means of two variables and lets us know if they belong to the same distribution. They also provide most efficient result."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy.stats import kstest\n",
        "\n",
        "v = np.random.normal(size=100)\n",
        "\n",
        "res = kstest(v, 'norm')\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KS test is used to check if given values follow a distribution.\n",
        "\n",
        "The function takes the value to be tested, and the CDF as two parameters."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For more efficient result.**"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy.stats import describe\n",
        "\n",
        "v = np.random.normal(size=100)\n",
        "res = describe(v)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "v = np.random.normal(size=100)\n",
        "\n",
        "print(skew(v))\n",
        "print(kurtosis(v))"
      ],
      "metadata": {
        "id": "3MEiNrwEc2Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "v = np.random.normal(size=100)\n",
        "\n",
        "print(normaltest(v))"
      ],
      "metadata": {
        "id": "HvMrOqtpcSHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I have perform normality test**"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normality tests are based on the skewness and kurtosis.\n",
        "\n",
        "The normaltest() function returns p value for the null hypothesis:\n",
        "\n",
        "\"x comes from a normal distribution\"."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "print(df.isnull().sum())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace= True)\n",
        "print(df.isnull().sum())\n",
        "print(df.shape)\n"
      ],
      "metadata": {
        "id": "0wQFN5UZXaDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msno.matrix(df)"
      ],
      "metadata": {
        "id": "lt2Ii4eoXybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "outliers = []\n",
        "def Finding_outliers(data):\n",
        "    data = sorted(data)\n",
        "    q1 = np.percentile(data, 25)\n",
        "    q3 = np.percentile(data, 75)\n",
        "    IQR = q3-q1\n",
        "    lwr_bound = q1-(1.5*IQR)\n",
        "    upr_bound = q3+(1.5*IQR)\n",
        "    for i in data:\n",
        "        if (i<lwr_bound or i>upr_bound):\n",
        "            outliers.append(i)\n",
        "    return outliers"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers = Finding_outliers(df['LIMIT_BAL'])"
      ],
      "metadata": {
        "id": "BVYyF2GGo9Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(outliers)"
      ],
      "metadata": {
        "id": "emnK-WFvpFBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median = np.median(df['LIMIT_BAL'])# Replace with median\n",
        "for i in outliers:\n",
        "    c = np.where(df['LIMIT_BAL']==i, median, df['LIMIT_BAL'])\n",
        "print(\"New array: \",c)\n",
        "#df['LIMIT_BAL']=c"
      ],
      "metadata": {
        "id": "1Xerfp76pPmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(c)"
      ],
      "metadata": {
        "id": "h4bpYGrfphUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['LIMIT_BAL'].describe()"
      ],
      "metadata": {
        "id": "RhfwohY5pq3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing 10th, 90th percentiles and replacing the outliers\n",
        "sample=df['LIMIT_BAL']\n",
        "tenth_percentile = np.percentile(sample, 10)\n",
        "ninetieth_percentile = np.percentile(sample, 90)\n",
        "print(tenth_percentile, ninetieth_percentile)\n",
        "b = np.where(sample<tenth_percentile, tenth_percentile, sample)\n",
        "b = np.where(b>ninetieth_percentile, ninetieth_percentile, b)\n",
        "print(\"New array:\",b)"
      ],
      "metadata": {
        "id": "O36SIoaFp5OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(b)"
      ],
      "metadata": {
        "id": "e0pDoq3TqXwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(b)"
      ],
      "metadata": {
        "id": "1_TXTc_-qbzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Considering option2 =>\n",
        "df['LIMIT_BAL']=c\n",
        "df['LIMIT_BAL']"
      ],
      "metadata": {
        "id": "JbaELCIzqlRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Limit_bin'] = pd.cut(df['LIMIT_BAL'],bins=[5000, 50000, 100000, 150000, 200000, 300000, 400000, 500000, 1100000])"
      ],
      "metadata": {
        "id": "MFtdQix0rBHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Limit_bin'].value_counts()"
      ],
      "metadata": {
        "id": "Fg3oq4tIsN6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "ax = sns.barplot(x = \"Limit_bin\", y = \"default payment next month\", data = df, palette = 'rocket_r', ci = None)\n",
        "plt.xlabel(\"Amount of Given Credit(LIMIT_BAL)\", fontsize= 12)\n",
        "plt.ylabel(\"% of Default\", fontsize= 12)\n",
        "plt.ylim(0,0.5)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.25, p.get_height()+0.03),fontsize=13)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JhePv7eysW_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can handle outliers in 3 methods.\n",
        "\n",
        "Removing/deleting the outliers,\n",
        "Replacing them with mean/median,\n",
        "Quantile based flooring and capping."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df['MARRIAGE'].nunique()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_features=df.columns"
      ],
      "metadata": {
        "id": "uInWP4aN5ziP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df['MARRIAGE'])[0:5]"
      ],
      "metadata": {
        "id": "MTOjFVWl36mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features=['AGE','EDUCATION','SEX']"
      ],
      "metadata": {
        "id": "8WbBgUZ54TOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df=pd.get_dummies(df[X_features],columns = categorical_features,drop_first=True)"
      ],
      "metadata": {
        "id": "wcyMmQMR42LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df.columns"
      ],
      "metadata": {
        "id": "petbd92o6CaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_features=encoded_df.columns"
      ],
      "metadata": {
        "id": "pZiWGqxO6aMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualitative variables orcategorical variables need to be encoded using dummy variables before incoprating in regression model."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import contractions\n",
        "# contracted text\n",
        "text = '''Pair plots are used to determine the most distinct clusters or the best combination of features to describe a connection between two variables.\n",
        "              By creating some straightforward linear separations or basic lines in our data set, it also helps to create some straightforward classification models.    '''\n",
        "\n",
        "# creating an empty list\n",
        "expanded_words = []\n",
        "for word in text.split():\n",
        "  # using contractions.fix to expand the shortened words\n",
        "  expanded_words.append(contractions.fix(word))\n",
        "\n",
        "expanded_text = ' '.join(expanded_words)\n",
        "print('Original text: ' + text)\n",
        "print('Expanded_text: ' + expanded_text)"
      ],
      "metadata": {
        "id": "lrdNJTmOhKNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Lower Casing\n",
        "string = \"CREDIT CARD DEFAULT PREDICTION !\"\n",
        "print(string.lower())\n",
        "\n",
        "# string with numbers\n",
        "# all alphabets should be lowercase\n",
        "string = \"CR3D!T CARD D3FAULT PR3DICT!ON!\"\n",
        "print(string.lower())"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Remove Punctuations\n",
        "# initializing string\n",
        "test_str = \"Credit, card : default  ! Prediction ;\"\n",
        "\n",
        "# printing original string\n",
        "print(\"The original string is : \" + test_str)\n",
        "\n",
        "# initializing punctuations string\n",
        "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "# Removing punctuations in string\n",
        "# Using loop + punctuation string\n",
        "for ele in test_str:\n",
        "    if ele in punc:\n",
        "        test_str = test_str.replace(ele, \"\")\n",
        "\n",
        "# printing result\n",
        "print(\"The string after punctuation filter : \" + test_str)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "t =\"This is a text with a URL https://www.java2blog.com/ to remove.\"\n",
        "s1 = re.sub('http://\\S+|https://\\S+', '', t)\n",
        "print(s1)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# import regex\n",
        "import re\n",
        "\n",
        "# download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# import nltk for stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# input string\n",
        "string = \"     This is a result of a virtuous circle: people who pay on duly tend to have better credit scores, so the banks prefer to increase these people’s credit lines by taking less risk.\"\n",
        "\n",
        "# convert to lower case\n",
        "lower_string = string.lower()\n",
        "\n",
        "# remove numbers\n",
        "no_number_string = re.sub(r'\\d+','',lower_string)\n",
        "\n",
        "# remove all punctuation except words and space\n",
        "no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)\n",
        "\n",
        "# remove white spaces\n",
        "no_wspace_string = no_punc_string.strip()\n",
        "no_wspace_string\n",
        "\n",
        "# convert string to list of words\n",
        "lst_string = [no_wspace_string][0].split()\n",
        "print(lst_string)\n",
        "\n",
        "# remove stopwords\n",
        "no_stpwords_string=\"\"\n",
        "for i in lst_string:\n",
        "    if not i in stop_words:\n",
        "        no_stpwords_string += i+' '\n",
        "\n",
        "# removing last space\n",
        "no_stpwords_string = no_stpwords_string[:-1]\n",
        "\n",
        "# output\n",
        "print(no_stpwords_string)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good. Because this technique gives more efficient result.**"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "no_stpwords_string=\"\"\n",
        "for i in lst_string:\n",
        "    if not i in stop_words:\n",
        "        no_stpwords_string += i+' '\n",
        "        # removing last space\n",
        "no_stpwords_string = no_stpwords_string[:-1]"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "no_wspace_string = no_punc_string.strip()\n",
        "no_wspace_string"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "! pip install transformers"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "text = \"\"\" This is a result of a virtuous circle: people who pay on duly tend to have better credit scores, so the banks prefer to increase these people’s credit lines by taking less risk.  \"\"\"\n",
        "data = text.split('.')\n",
        "for i in data:\n",
        "    print (i)\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "sentence = \"This is a result of a virtuous circle.\\\n",
        " people who pay on duly tend to have better credit scores\"\n",
        "for token in nlp(sentence):\n",
        "    print(f'{token.text:{10}} {token.tag_:>{10}}\\t{spacy.explain(token.tag_):<{50}} {token.pos_:>{5}}')"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "cv = CountVectorizer()\n",
        "sentence = ['This is a result of a virtuous circle' , ' people who pay on duly tend to have better credit scores']\n",
        "bow_rep = cv.fit_transform(sentence)\n",
        "\n",
        "pp.pprint(cv.vocabulary_)\n",
        "\n",
        "print(\"BoW representaion for {}:\".format(sentence[0]),bow_rep[0].toarray())\n",
        "print(\"BoW representaion for {}:\".format(sentence[1]),bow_rep[1].toarray())"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OHE(text):\n",
        "    tokens = set(text.lower().split())\n",
        "    length = len(tokens)\n",
        "    index_map = {x:index for x,index in zip(tokens,range(length))}\n",
        "    ohe_matrix = []\n",
        "\n",
        "    for token in tokens:\n",
        "                ohe = np.zeros(length)\n",
        "                ohe[index_map[token]] = 1\n",
        "                print(token,ohe)\n",
        "                ohe_matrix.append(ohe)\n",
        "OHE('This is a result of a virtuous circle')"
      ],
      "metadata": {
        "id": "nDz5nyXtjNPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.**"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ### 4. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "Y2_QwLdWXgum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes the dataset is imbalanced .**"
      ],
      "metadata": {
        "id": "X83FDLSGYng1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "smote = SMOTE()\n",
        "\n",
        "# fit predictor and target variable\n",
        "x_smote, y_smote = smote.fit_resample(df.iloc[:,0:-1], df['default payment next month'])\n",
        "\n",
        "print('Original shape of Dataset', len(df))\n",
        "print('Resampled shape of Dataset', len(y_smote))"
      ],
      "metadata": {
        "id": "I4DImXJjYuYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = list(df.columns)\n",
        "columns.pop()"
      ],
      "metadata": {
        "id": "s9Z9DrUNZAlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFQSg-kE-xt9"
      },
      "outputs": [],
      "source": [
        "balance_df = pd.DataFrame(x_smote, columns=columns)\n",
        "balance_df['default payment next month'] = y_smote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFByiLmo-1bP"
      },
      "outputs": [],
      "source": [
        "balance_df[balance_df['default payment next month']==1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TUB4PfXaaPBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I have used **SMOTE (Synthetic Minority Oversampling Technique)**"
      ],
      "metadata": {
        "id": "cn8rO_p3aWIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import seaborn as sns\n",
        "import matplotlib. pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#  Correlation matrix\n",
        "cor = df.corr()\n",
        "\n",
        "#Plotting Heatmap\n",
        "plt.figure(figsize =( 20,16))\n",
        "sns.heatmap(cor, annot=True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "df_fr = balance_df.copy()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr['Payement_Value'] = df_fr['PAY_SEPT'] + df_fr['PAY_AUG'] + df_fr['PAY_JUL'] + df_fr['PAY_JUN'] + df_fr['PAY_MAY'] + df_fr['PAY_APR']\n",
        "\n",
        "df_fr.groupby('default payment next month')['Payement_Value'].mean()"
      ],
      "metadata": {
        "id": "sVb2wk_b_qOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,7))\n",
        "sns.boxplot(data = df_fr, x = 'default payment next month', y = 'Payement_Value' )"
      ],
      "metadata": {
        "id": "pVQZBAm3_vJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79eXzf7Z_QZv"
      },
      "outputs": [],
      "source": [
        "df_fr['Dues'] = (df_fr['BILL_AMT_APR']+df_fr['BILL_AMT_MAY']+df_fr['BILL_AMT_JUN']+df_fr['BILL_AMT_JUL']+df_fr['BILL_AMT_SEPT'])-(df_fr['PAY_AMT_APR']+df_fr['PAY_AMT_MAY']+df_fr['PAY_AMT_JUN']+df_fr['PAY_AMT_JUL']+df_fr['PAY_AMT_AUG']+df_fr['PAY_AMT_SEPT'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr.groupby('default payment next month')['Dues'].mean()"
      ],
      "metadata": {
        "id": "3a5eMaWyAAoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr['EDUCATION']=np.where(df_fr['EDUCATION'] == 6, 4, df_fr['EDUCATION'])\n",
        "df_fr['EDUCATION']=np.where(df_fr['EDUCATION'] == 0, 4, df_fr['EDUCATION'])"
      ],
      "metadata": {
        "id": "3Uh_TIIuAGhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_fr['EDUCATION'].unique())\n",
        "print(df_fr['MARRIAGE'].unique())"
      ],
      "metadata": {
        "id": "hu4TCQ2aALfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr['MARRIAGE']=np.where(df_fr['MARRIAGE'] == 0, 3, df_fr['MARRIAGE'])"
      ],
      "metadata": {
        "id": "EPddPDyKARaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpfpzaOD_8bf"
      },
      "outputs": [],
      "source": [
        "df_fr.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'}, 'EDUCATION' : {1 : 'graduate school', 2 : 'university', 3 : 'high school', 4 : 'others'}, 'MARRIAGE' : {1 : 'married', 2 : 'single', 3 : 'others'}}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr.describe().transpose()"
      ],
      "metadata": {
        "id": "JtwzKcEFATVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr.head(7)"
      ],
      "metadata": {
        "id": "4za-6KqCAyc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used **\"correlation cofficient\"**.\n",
        "\n",
        "Because Correlation is a measure of the linear relationship between 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that good variables correlate highly with the target. Furthermore, variables should be correlated with the target but uncorrelated among themselves."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got all the details about the columns and rows ."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "#function to scale\n",
        "def do_scale(X_train, X_test, scaling_type = StandardScaler):\n",
        "  scaler = scaling_type()\n",
        "  scaler.fit(X_train)\n",
        "  X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
        "  X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
        "  return X_train_scaled, X_test_scaled"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No i  don't think that dimensionality reduction is needed.**"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "df_fr = pd.get_dummies(df_fr, columns=['EDUCATION','MARRIAGE'])"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr.drop(['EDUCATION_others','MARRIAGE_others'],axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "9Ethqne82l3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr = pd.get_dummies(df_fr, columns = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR'], drop_first = True )"
      ],
      "metadata": {
        "id": "xmg_46CH3V-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LABEL ENCODING FOR SEX\n",
        "encoders_nums = {\n",
        "                 \"SEX\":{\"FEMALE\": 0, \"MALE\": 1}\n",
        "}\n",
        "df_fr = df_fr.replace(encoders_nums)"
      ],
      "metadata": {
        "id": "b12ZUn6h3bCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr.drop('ID',axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "VtEAOttL3dKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr.head()"
      ],
      "metadata": {
        "id": "O9vfnD373huU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fr.shape"
      ],
      "metadata": {
        "id": "eaPc2P463mC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "5vfF5gWU3sQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_log_reg = df_fr.copy()"
      ],
      "metadata": {
        "id": "YNJiv8D-311N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_log_reg.drop(['default payment next month','Payement_Value','Dues'],axis=1)\n",
        "y = df_log_reg['default payment next month']"
      ],
      "metadata": {
        "id": "TrBZBqyc37E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = X.columns"
      ],
      "metadata": {
        "id": "iL8eFQD_4Acj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "xEhHriNh4JZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "98vZdcHd4P_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'penalty':['l1','l2'], 'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000] }"
      ],
      "metadata": {
        "id": "fwDtjgrs4VTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_lr_clf = GridSearchCV(LogisticRegression(), param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "grid_lr_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "suDHXnwN4XQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_clf = grid_lr_clf.best_estimator_"
      ],
      "metadata": {
        "id": "4m6J-X5g4hBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_lr_clf.best_params_"
      ],
      "metadata": {
        "id": "Nhlb5_rp4jHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_lr_clf.best_score_"
      ],
      "metadata": {
        "id": "PUze4XNa4o82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted Probability\n",
        "train_preds = optimized_clf.predict_proba(X_train)[:,1]\n",
        "test_preds = optimized_clf.predict_proba(X_test)[:,1]"
      ],
      "metadata": {
        "id": "KGDpNGwm4xnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimized_clf.predict(X_train)\n",
        "test_class_preds = optimized_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "oEGU3j6W44CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**80 to 20 ratio is used . Because it gives most efficient result.**"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # MODEL EVALUATION"
      ],
      "metadata": {
        "id": "FoD9ojAN7YcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "\n",
        "\n",
        "# Get the accuracy scores\n",
        "train_accuracy_lr = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_lr = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_lr)\n",
        "print(\"The accuracy on test data is \", test_accuracy_lr)\n",
        "\n"
      ],
      "metadata": {
        "id": "tmqw85eE8TGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_lr = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_lr = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_lr = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_lr = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_lr = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_lr)\n",
        "print(\"The precision on test data is \", test_precision_score_lr)\n",
        "print(\"The recall on test data is \", test_recall_score_lr)\n",
        "print(\"The f1 on test data is \", test_f1_score_lr)\n",
        "print(\"The roc_score on test data is \", test_roc_score_lr)"
      ],
      "metadata": {
        "id": "6v3jgNKB8fye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFUSION MATRIX"
      ],
      "metadata": {
        "id": "8uF8Rxv48qWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "KkvkqzUd8nuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame({'Features':columns, 'Importance':np.abs(optimized_clf.coef_).ravel() })"
      ],
      "metadata": {
        "id": "p_Y5Bhqh8y7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = feature_importance.sort_values(by = 'Importance', ascending=False)[:10]"
      ],
      "metadata": {
        "id": "2Qmoxvig85CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(height=feature_importance['Importance'], x= feature_importance['Features'])\n",
        "plt.xticks(rotation=50)\n",
        "plt.title(\"Feature importances via coefficients\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pdf8otbJ9A8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omp_tsS0xPya"
      },
      "source": [
        "ROC AUC curve"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_proba_lr = optimized_clf.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "8FaYVh0z9lvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = y_preds_proba_lr\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uoS5lsEI9THg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm2Rri7bxf6l"
      },
      "source": [
        "# Implementing SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter Range\n",
        "param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'kernel': ['rbf']}"
      ],
      "metadata": {
        "id": "oKlp5ebr90sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_fr.drop(['default payment next month','Payement_Value','Dues'],axis=1)\n",
        "y = df_fr['default payment next month']"
      ],
      "metadata": {
        "id": "ik_2bWSC99YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Yb77ODNv99AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "Sike23cW-KEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_clf = GridSearchCV(SVC(probability=True), param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "grid_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "CE3cPyDm-J3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_SVC_clf = grid_clf.best_estimator_"
      ],
      "metadata": {
        "id": "XY_r2dzr-pNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "grid_clf.best_params_\n",
        "\n"
      ],
      "metadata": {
        "id": "DsOF4yxa-16N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_clf.best_score_"
      ],
      "metadata": {
        "id": "MmyiHsZS-1td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_SVC_clf.predict(X_train)\n",
        "test_class_preds = optimal_SVC_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "SfblNEzD-1fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_SVC = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_SVC = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_lr)\n",
        "print(\"The accuracy on test data is \", test_accuracy_lr)"
      ],
      "metadata": {
        "id": "M_hOg8Tu-1Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_SVC = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_SVC = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_SVC = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_SVC = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_SVC = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_SVC)\n",
        "print(\"The precision on test data is \", test_precision_score_SVC)\n",
        "print(\"The recall on test data is \", test_recall_score_SVC)\n",
        "print(\"The f1 on test data is \", test_f1_score_SVC)\n",
        "print(\"The roc_score on test data is \", test_roc_score_SVC)"
      ],
      "metadata": {
        "id": "BXkIP6rHM5eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix for both train and test\n",
        "\n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_train, train_class_preds)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "H2dy3Vi9M5Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_SVC_clf"
      ],
      "metadata": {
        "id": "pfdFh4kxNOY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_SVC_clf.predict(X_train)\n",
        "test_class_preds = optimal_SVC_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "Zj9KqwxpM5GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wcP_NoOyDdA"
      },
      "source": [
        "ROC AUC curve"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba_SVC = optimal_SVC_clf.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "HAh2MpnbN8Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ROC AUC CURVE\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_pred_proba_SVC)\n",
        "auc = roc_auc_score(y_test, y_pred_proba_SVC)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7l9TkmY9N7xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'max_depth': [20,30,50,100], 'min_samples_split':[0.1,0.2,0.4]}"
      ],
      "metadata": {
        "id": "0F4ABvu8Pp8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_fr.drop(['default payment next month','Payement_Value','Dues'],axis=1)\n",
        "y = df_fr['default payment next month']"
      ],
      "metadata": {
        "id": "x6XSN5yYPspO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "tns-pl0ZP6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_DTC_clf = GridSearchCV(DecisionTreeClassifier(), param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "grid_DTC_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-Lt2OzH8QCQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_DTC_clf.best_score_"
      ],
      "metadata": {
        "id": "Q_nSILRlQCDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_DTC_clf = grid_DTC_clf.best_estimator_"
      ],
      "metadata": {
        "id": "VMIh5dkSQB3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_DTC_clf.predict(X_train)\n",
        "test_class_preds = optimal_DTC_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "1ZbpbdA9Qd0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_DTC_clf.best_params_"
      ],
      "metadata": {
        "id": "mn2bHwDdQdnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_DTC = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_DTC = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_DTC)\n",
        "print(\"The accuracy on test data is \", test_accuracy_DTC)"
      ],
      "metadata": {
        "id": "iU9Wi0I4Qntj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_hastie_10_2(n_samples=8000, random_state=42)\n",
        "\n",
        "# The scorers can be either one of the predefined metric strings or a scorer\n",
        "# callable, like the one returned by make_scorer\n",
        "scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": make_scorer(accuracy_score)}\n",
        "\n",
        "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
        "# parameter setting that has the best cross-validated AUC score.\n",
        "# That estimator is made available at ``gs.best_estimator_`` along with\n",
        "# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n",
        "# ``gs.best_index_``\n",
        "gs = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid={\"min_samples_split\": range(2, 403, 20)},\n",
        "    scoring=scoring,\n",
        "    refit=\"AUC\",\n",
        "    n_jobs=2,\n",
        "    return_train_score=True,\n",
        ")\n",
        "gs.fit(X, y)\n",
        "results = gs.cv_results_"
      ],
      "metadata": {
        "id": "F3sq-avGvv7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13, 13))\n",
        "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n",
        "\n",
        "plt.xlabel(\"min_samples_split\")\n",
        "plt.ylabel(\"Score\")\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set_xlim(0, 402)\n",
        "ax.set_ylim(0.73, 1)\n",
        "\n",
        "# Get the regular numpy array from the MaskedArray\n",
        "X_axis = np.array(results[\"param_min_samples_split\"].data, dtype=float)\n",
        "\n",
        "for scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\n",
        "    for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
        "        sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\n",
        "        sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\n",
        "        ax.fill_between(\n",
        "            X_axis,\n",
        "            sample_score_mean - sample_score_std,\n",
        "            sample_score_mean + sample_score_std,\n",
        "            alpha=0.1 if sample == \"test\" else 0,\n",
        "            color=color,\n",
        "                 )\n",
        "        ax.plot(\n",
        "            X_axis,\n",
        "            sample_score_mean,\n",
        "            style,\n",
        "            color=color,\n",
        "            alpha=1 if sample == \"test\" else 0.7,\n",
        "            label=\"%s (%s)\" % (scorer, sample),\n",
        "        )\n",
        "\n",
        "    best_index = np.nonzero(results[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
        "    best_score = results[\"mean_test_%s\" % scorer][best_index]\n",
        "\n",
        "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
        "    ax.plot(\n",
        "        [\n",
        "            X_axis[best_index],\n",
        "        ]\n",
        "        * 2,\n",
        "        [0, best_score],\n",
        "        linestyle=\"-.\",\n",
        "        color=color,\n",
        "        marker=\"x\",\n",
        "        markeredgewidth=3,\n",
        "        ms=8,\n",
        "    )\n",
        "\n",
        "    # Annotate the best score for that scorer\n",
        "    ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
        "\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QjnrrLwrv3SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "# predict the model\n",
        "\n",
        "# sets of hyperparameters\n",
        "params_1 = {'criterion': 'gini', 'splitter': 'best', 'max_depth': 50}\n",
        "params_2 = {'criterion': 'entropy', 'splitter': 'random', 'max_depth': 70}\n",
        "params_3 = {'criterion': 'gini', 'splitter': 'random', 'max_depth': 60}\n",
        "params_4 = {'criterion': 'entropy', 'splitter': 'best', 'max_depth': 80}\n",
        "params_5 = {'criterion': 'gini', 'splitter': 'best', 'max_depth': 40}\n",
        "# Separate models\n",
        "model_1 = DecisionTreeClassifier(**params_1)\n",
        "model_2 = DecisionTreeClassifier(**params_2)\n",
        "model_3 = DecisionTreeClassifier(**params_3)\n",
        "model_4 = DecisionTreeClassifier(**params_4)\n",
        "model_5 = DecisionTreeClassifier(**params_5)\n",
        "model_1.fit(X_train, y_train)\n",
        "model_2.fit(X_train, y_train)\n",
        "model_3.fit(X_train, y_train)\n",
        "model_4.fit(X_train, y_train)\n",
        "model_5.fit(X_train, y_train)\n",
        "# Prediction sets\n",
        "preds_1 = model_1.predict(X_test)\n",
        "preds_2 = model_3.predict(X_test)\n",
        "preds_3 = model_3.predict(X_test)\n",
        "preds_4 = model_4.predict(X_test)\n",
        "preds_5 = model_5.predict(X_test)\n",
        "print(f'Accuracy on Model 1: {round(accuracy_score(y_test, preds_1), 3)}')\n",
        "print(f'Accuracy on Model 2: {round(accuracy_score(y_test, preds_2), 3)}')\n",
        "print(f'Accuracy on Model 3: {round(accuracy_score(y_test, preds_3), 3)}')\n",
        "print(f'Accuracy on Model 4: {round(accuracy_score(y_test, preds_4), 3)}')\n",
        "print(f'Accuracy on Model 5: {round(accuracy_score(y_test, preds_5), 3)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I have used gridsearch .Because it gives the best and the most prominent result.**"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes i have seen the improvement. The improvement in the metric score is shown as **\n",
        "\n",
        "Accuracy on Model 1: 0.745\n",
        "\n",
        "Accuracy on Model 2: 0.744\n",
        "\n",
        "Accuracy on Model 3: 0.744\n",
        "\n",
        "Accuracy on Model 4: 0.747\n",
        "\n",
        "Accuracy on Model 5: 0.744\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_fr.drop(['default payment next month','Payement_Value','Dues'],axis=1)\n",
        "y = df_fr['default payment next month']"
      ],
      "metadata": {
        "id": "6YCE8TzoyrwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = RandomForestClassifier()\n",
        "rf_clf.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "9ig6W7gfyuPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = rf_clf.predict(X_train)\n",
        "test_class_preds = rf_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "QEgXBoaJyuDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_rf = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_rf)\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)"
      ],
      "metadata": {
        "id": "FTc0CgI4yt2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_rf = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_rf = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_rf = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_rf = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)\n",
        "print(\"The precision on test data is \", test_precision_score_rf)\n",
        "print(\"The recall on test data is \", test_recall_score_rf)\n",
        "print(\"The f1 on test data is \", test_f1_score_rf)\n",
        "print(\"The roc_score on test data is \", test_roc_score_rf)"
      ],
      "metadata": {
        "id": "13igwDGgy61O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see from above results that we are getting around 99% train accuracy and 83% for test accuracy which depicts that model is overfitting. However our f1-score is around 82%, which is not bad**"
      ],
      "metadata": {
        "id": "mg055wRMzsBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [100,150,200], 'max_depth': [10,20,30]}"
      ],
      "metadata": {
        "id": "3WSZJILNzW4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_rf_clf = GridSearchCV(RandomForestClassifier(), param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "grid_rf_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-lnC58Z60Je6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_rf_clf.best_score_"
      ],
      "metadata": {
        "id": "4GaYJlcY0Jbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_rf_clf.best_params_"
      ],
      "metadata": {
        "id": "K5OxgA7x0JYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "optimal_rf_clf = grid_rf_clf.best_estimator_\n",
        "\n"
      ],
      "metadata": {
        "id": "-a3_NNfO0JU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_rf_clf.predict(X_train)\n",
        "test_class_preds = optimal_rf_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "BfE9PGLw0JP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_rf = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_rf)\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)"
      ],
      "metadata": {
        "id": "89VZ8g1G0JHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_accuracy_rf = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_score_rf = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_rf = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_rf = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_rf = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_rf)\n",
        "print(\"The precision on test data is \", test_precision_score_rf)\n",
        "print(\"The recall on test data is \", test_recall_score_rf)\n",
        "print(\"The f1 on test data is \", test_f1_score_rf)\n",
        "print(\"The roc_score on test data is \", test_roc_score_rf)\n",
        "\n"
      ],
      "metadata": {
        "id": "zsfY2GRw0I-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(optimal_rf_clf.feature_importances_)"
      ],
      "metadata": {
        "id": "6Sq6otVX2r-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Feature Importance\n",
        "feature_importances_rf = pd.DataFrame(optimal_rf_clf.feature_importances_,\n",
        "                                   index = columns,\n",
        "                                    columns=['importance_rf']).sort_values('importance_rf',\n",
        "                                                                        ascending=False)[:10]\n",
        "\n",
        "plt.subplots(figsize=(17,6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(feature_importances_rf.index, feature_importances_rf['importance_rf'],\n",
        "        color=\"pink\",  align=\"center\")\n",
        "plt.xticks(feature_importances_rf.index, rotation = 85)\n",
        "#plt.xlim([-1, X.shape[1]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_rf_clf.predict(X_train)\n",
        "test_class_preds = optimal_rf_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "wGphIzQs206Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_proba_rf = optimal_rf_clf.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "tiIso2Qs_-i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_proba = y_preds_proba_rf\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "zufEXrQM27cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
        "dtest=xgb.DMatrix(X_test)"
      ],
      "metadata": {
        "id": "J3zDWsGG3cc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}"
      ],
      "metadata": {
        "id": "gqs9zEWD3iY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training our model\n",
        "num_round=50\n",
        "from datetime import datetime\n",
        "start = datetime.now()\n",
        "xg=xgb.train(parameters,dtrain,num_round)\n",
        "stop = datetime.now()"
      ],
      "metadata": {
        "id": "NFlqfJPH3iU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execution time of the model\n",
        "execution_time_xgb = stop-start\n",
        "execution_time_xgb"
      ],
      "metadata": {
        "id": "83VnZQsB3iR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now predicting our model on train set\n",
        "train_class_preds_probs=xg.predict(dtrain)\n",
        "#now predicting our model on test set\n",
        "test_class_preds_probs =xg.predict(dtest)"
      ],
      "metadata": {
        "id": "clup9GIh3iOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_class_preds_probs)"
      ],
      "metadata": {
        "id": "2OnrWwiP3iKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_preds = []\n",
        "test_class_preds = []\n",
        "for i in range(0,len(train_class_preds_probs)):\n",
        "  if train_class_preds_probs[i] >= 0.5:\n",
        "    train_class_preds.append(1)\n",
        "  else:\n",
        "    train_class_preds.append(0)\n",
        "\n",
        "for i in range(0,len(test_class_preds_probs)):\n",
        "  if test_class_preds_probs[i] >= 0.5:\n",
        "    test_class_preds.append(1)\n",
        "  else:\n",
        "    test_class_preds.append(0)"
      ],
      "metadata": {
        "id": "9UNExxM04O-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_class_preds_probs[:20]"
      ],
      "metadata": {
        "id": "Ivx5BnVH4WJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_class_preds[:20]"
      ],
      "metadata": {
        "id": "2pIoK1dZ4Y09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_train)"
      ],
      "metadata": {
        "id": "Dt6c16HE4d6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_class_preds)"
      ],
      "metadata": {
        "id": "K2tpUwZ14iUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_xgb = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_xgb = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_xgb)\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb)"
      ],
      "metadata": {
        "id": "IVSiJ_bV4nqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_accuracy_xgb = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_xgb = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_xgb = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_xgb = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_xgb = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb)\n",
        "print(\"The precision on test data is \", test_precision_xgb)\n",
        "print(\"The recall on test data is \", test_recall_score_xgb)\n",
        "print(\"The f1 on test data is \", test_f1_score_xgb)\n",
        "print(\"The roc_score on train data is \", test_roc_score_xgb)\n",
        "\n"
      ],
      "metadata": {
        "id": "i8c31gMg4sfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "7PWRQXqa5Nzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import  XGBClassifier"
      ],
      "metadata": {
        "id": "k3lD2tP75Saw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_fr.drop(['default payment next month','Payement_Value','Dues'],axis=1)\n",
        "y = df_fr['default payment next month']"
      ],
      "metadata": {
        "id": "EClBbvPh5XDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "dz7kAhwd5fMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_test1 = {\n",
        " 'max_depth':range(3,10,2),\n",
        " 'min_child_weight':range(1,6,2)\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
        " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
        " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
        " param_grid = param_test1, scoring='accuracy',n_jobs=-1, cv=3, verbose = 2)\n",
        "gsearch1.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ip5wMrvA5hNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsearch1.best_score_"
      ],
      "metadata": {
        "id": "coQa6ALX6DZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_xgb = gsearch1.best_estimator_optimal_xgb = gsearch1.best_estimator_"
      ],
      "metadata": {
        "id": "XqA1SjQI6DLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "train_class_preds = optimal_xgb.predict(X_train)\n",
        "test_class_preds = optimal_xgb.predict(X_test)"
      ],
      "metadata": {
        "id": "Sc75b7D66VfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the accuracy scores\n",
        "train_accuracy_xgb_tuned = accuracy_score(train_class_preds,y_train)\n",
        "test_accuracy_xgb_tuned = accuracy_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on train data is \", train_accuracy_xgb_tuned)\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb_tuned)"
      ],
      "metadata": {
        "id": "YOx7yMTA7R9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_xgb_tuned = accuracy_score(test_class_preds,y_test)\n",
        "test_precision_xgb_tuned = precision_score(test_class_preds,y_test)\n",
        "test_recall_score_xgb_tuned = recall_score(test_class_preds,y_test)\n",
        "test_f1_score_xgb_tuned = f1_score(test_class_preds,y_test)\n",
        "test_roc_score_xgb_tuned = roc_auc_score(test_class_preds,y_test)\n",
        "\n",
        "print(\"The accuracy on test data is \", test_accuracy_xgb_tuned)\n",
        "print(\"The precision on test data is \", test_precision_xgb_tuned)\n",
        "print(\"The recall on test data is \", test_recall_score_xgb_tuned)\n",
        "print(\"The f1 on test data is \", test_f1_score_xgb_tuned)\n",
        "print(\"The roc_score on train data is \", test_roc_score_xgb_tuned)"
      ],
      "metadata": {
        "id": "G9QSntdt7Yl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(optimal_xgb.feature_importances_,index = columns,columns=['importance_xgb']).sort_values('importance_xgb',ascending=False)[:10]"
      ],
      "metadata": {
        "id": "w_n0O74H7_D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Feature Importance\n",
        "feature_importances_xgb = pd.DataFrame(optimal_xgb.feature_importances_,index = columns,columns=['importance_xgb']).sort_values('importance_xgb',ascending=False)[:10]\n",
        "\n",
        "plt.subplots(figsize=(17,6))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(feature_importances_xgb.index, feature_importances_xgb['importance_xgb'],\n",
        "        color=\"r\",  align=\"center\")\n",
        "# plt.xticks(feature_importances_rf.index, rotation = 85)\n",
        "#plt.xlim([-1, X.shape[1]])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "gm8ecxwE7-t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds_proba_xgb = optimal_xgb.predict_proba(X_test)[::,1]"
      ],
      "metadata": {
        "id": "yulJUb468Z7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = y_preds_proba_xgb\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TBUan0eQ8Zra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the models"
      ],
      "metadata": {
        "id": "punj7Nqg9UDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score"
      ],
      "metadata": {
        "id": "cnt6cjdf9dqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "classifiers = ['Logistic Regression', 'SVC', 'Random Forest CLf', 'Xgboost Clf']\n",
        "train_accuracy = [train_accuracy_lr, train_accuracy_SVC, train_accuracy_rf, train_accuracy_xgb_tuned]\n",
        "test_accuracy = [test_accuracy_lr, test_accuracy_SVC, test_accuracy_rf, test_accuracy_xgb_tuned]\n",
        "precision_score = [test_precision_score_lr, test_precision_score_SVC, test_precision_score_rf, test_precision_xgb_tuned]\n",
        "recall_score = [test_recall_score_lr, test_recall_score_SVC, test_recall_score_rf, test_recall_score_xgb_tuned]\n",
        "f1_score = [test_f1_score_lr, test_f1_score_SVC, test_f1_score_rf, test_f1_score_xgb_tuned]\n",
        "\n"
      ],
      "metadata": {
        "id": "qZElmDAd9dcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pd.DataFrame({'Classifier':classifiers, 'Train Accuracy': train_accuracy, 'Test Accuracy': test_accuracy, 'Precision Score': precision_score, 'Recall Score': recall_score, 'F1 Score': f1_score })\n",
        "\n"
      ],
      "metadata": {
        "id": "HV5ssaCE9cwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plotting ROC AUC for all the models**"
      ],
      "metadata": {
        "id": "bAw7nMAk92aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers_proba = [(optimized_clf, y_preds_proba_lr),\n",
        "               (optimal_rf_clf, y_preds_proba_rf),\n",
        "               (optimal_xgb, y_preds_proba_xgb),\n",
        "               (optimal_SVC_clf,y_pred_proba_SVC)]\n",
        "\n",
        "# Define a result table as a DataFrame\n",
        "result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
        "\n",
        "# Train the models and record the results\n",
        "for pair in classifiers_proba:\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test,  pair[1])\n",
        "    auc = roc_auc_score(y_test, pair[1])\n",
        "\n",
        "    result_table = result_table.append({'classifiers':pair[0].__class__.__name__,\n",
        "                                        'fpr':fpr,\n",
        "                                        'tpr':tpr,\n",
        "                                        'auc':auc}, ignore_index=True)\n",
        "\n",
        "# Set name of the classifiers as index labels\n",
        "result_table.set_index('classifiers', inplace=True)"
      ],
      "metadata": {
        "id": "JFtVluJM93mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "for i in result_table.index:\n",
        "    plt.plot(result_table.loc[i]['fpr'],\n",
        "             result_table.loc[i]['tpr'],\n",
        "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
        "\n",
        "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"Flase Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size':13}, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W14H2OPN-aFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GRIDSEARCH"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YES THE IMPROVEMENT CAN BE SEEN AS IT IS ALSO SEEN IN THE METRIC SCORES"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBOOST**  has the  best evaluation metrics because of the recall balance ."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBOOST**\n",
        "Because it has  the best precision and recall balance."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used DECISIONTREE ,RANDOMFOREST AND XGBOOST."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ After performing the various model we the get the best accuracy form the Random forest and XGBoost classifier.\n",
        "\n",
        "▶ Logestic Regression is the least accurate as compared to other models performed.\n",
        "\n",
        "▶ XGBoost has the best precision and the recall balance.\n",
        "\n",
        "▶ Higher recall can be achieved if low precision is acceptable.\n",
        "\n",
        "▶ We can deploy the model and can be served as an aid to human decision.\n",
        "\n",
        "▶ Model can be improved with more data and computational resources"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}